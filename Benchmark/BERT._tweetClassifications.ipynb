{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "80694210-a933-4093-ade6-30830ecbd273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "381/381 [==============================] - 1032s 3s/step - loss: 0.4464 - accuracy: 0.8025 - val_loss: 0.3854 - val_accuracy: 0.8404\n",
      "Epoch 2/2\n",
      "381/381 [==============================] - 1022s 3s/step - loss: 0.3105 - accuracy: 0.8778 - val_loss: 0.4155 - val_accuracy: 0.8293\n",
      "96/96 [==============================] - 71s 735ms/step - loss: 0.4155 - accuracy: 0.8293\n",
      "Validation Accuracy: 0.8293\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification, BertTokenizer, create_optimizer\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and split data using skilearn \n",
    "#80% used for training from traning.csv and 20% for testing. \n",
    "df = pd.read_csv(\"train.csv\")\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"text\"].tolist(), df[\"target\"].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def create_dataset(texts, labels, tokenizer, batch_size=16):\n",
    "    tokens = tokenizer(texts, truncation=True, padding=True)\n",
    "    inputs = {\n",
    "        \"input_ids\": tf.convert_to_tensor(tokens[\"input_ids\"]),\n",
    "        \"attention_mask\": tf.convert_to_tensor(tokens[\"attention_mask\"]),\n",
    "       \n",
    "        \"token_type_ids\": tf.convert_to_tensor(tokens.get(\"token_type_ids\", [[0]*len(tokens[\"input_ids\"][0])] * len(texts))),\n",
    "        \"labels\": tf.convert_to_tensor(labels)\n",
    "    }\n",
    "    return tf.data.Dataset.from_tensor_slices(inputs).batch(batch_size)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = create_dataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = create_dataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "# Load pretrained BERT model for sequence classification\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Calculate steps and create optimizer\n",
    "steps_per_epoch = len(train_dataset)\n",
    "num_train_steps = steps_per_epoch * 2  # 2 epochs\n",
    "optimizer, _ = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=num_train_steps)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model with validation\n",
    "model.fit(train_dataset, epochs=2, validation_data=val_dataset)\n",
    "\n",
    "# Evaluate on validation set\n",
    "loss, accuracy = model.evaluate(val_dataset)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495180b0-c504-42d6-8eb2-49a8d4b44931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
